%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Programming/Coding Assignment
% LaTeX Template
%
% This template has been downloaded from:
% http://www.latextemplates.com
%
% Original author:
% Ted Pavlic (http://www.tedpavlic.com)
%
% Note:
% The \lipsum[#] commands throughout this template generate dummy text
% to fill the template out. These commands should all be removed when 
% writing assignment content.
%
% This template uses a Perl script as an example snippet of code, most other
% languages are also usable. Configure them in the "CODE INCLUSION 
% CONFIGURATION" section.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage[usenames,dvipsnames]{color} % Required for custom colors
\usepackage{graphicx} % Required to insert images
\usepackage{listings} % Required for insertion of code
\usepackage{courier} % Required for the courier font
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\usepackage[fleqn]{amsmath}
\usepackage{hyperref}
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}
% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{\hmwkAuthorName} % Top left header
\chead{\hmwkClass\ (\hmwkClassInstructor\ \hmwkClassTime): \hmwkTitle} % Top center head
\rhead{\firstxmark} % Top right header
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \protect\pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
%	CODE INCLUSION CONFIGURATION
%----------------------------------------------------------------------------------------

\definecolor{MyDarkGreen}{rgb}{0.0,0.4,0.0} % This is the color used for comments
\lstloadlanguages{Perl} % Load Perl syntax for listings, for a list of other languages supported see: ftp://ftp.tex.ac.uk/tex-archive/macros/latex/contrib/listings/listings.pdf
\lstset{language=Perl, % Use Perl in this example
        frame=single, % Single frame around code
        basicstyle=\small\ttfamily, % Use small true type font
        keywordstyle=[1]\color{Blue}\bf, % Perl functions bold and blue
        keywordstyle=[2]\color{Purple}, % Perl function arguments purple
        keywordstyle=[3]\color{Blue}\underbar, % Custom functions underlined and blue
        identifierstyle=, % Nothing special about identifiers                                         
        commentstyle=\usefont{T1}{pcr}{m}{sl}\color{MyDarkGreen}\small, % Comments small dark green courier font
        stringstyle=\color{Purple}, % Strings are purple
        showstringspaces=false, % Don't put marks in string spaces
        tabsize=5, % 5 spaces per tab
        %
        % Put standard Perl functions not included in the default language here
        morekeywords={rand},
        %
        % Put Perl function parameters here
        morekeywords=[2]{on, off, interp},
        %
        % Put user defined functions here
        morekeywords=[3]{test},
       	%
        morecomment=[l][\color{Blue}]{...}, % Line continuation (...) like blue comment
        numbers=left, % Line numbers on left
        firstnumber=1, % Line numbers start with line 1
        numberstyle=\tiny\color{Blue}, % Line numbers are blue and small
        stepnumber=5 % Line numbers go in steps of 5
}

\lstset{language=Matlab,%
    %basicstyle=\color{red},
    breaklines=true,%
    morekeywords={matlab2tikz},
    keywordstyle=\color{blue},%
    morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},
    identifierstyle=\color{black},%
    stringstyle=\color{mylilas},
    commentstyle=\color{mygreen},%
    showstringspaces=false,%without this there will be a symbol in the places where there is a space
    numbers=left,%
    numberstyle={\tiny \color{black}},% size of the numbers
    numbersep=9pt, % this defines how far the numbers are from the text
    emph=[1]{for,end,break},emphstyle=[1]\color{red}, %some words to emphasise
    %emph=[2]{word1,word2}, emphstyle=[2]{style},    
}

% Creates a new command to include a perl script, the first parameter is the filename of the script (without .pl), the second parameter is the caption
\newcommand{\perlscript}[2]{
\begin{itemize}
\item[]\lstinputlisting[caption=#2,label=#1]{#1.pl}
\end{itemize}
}


%----------------------------------------------------------------------------------------
%	DOCUMENT STRUCTURE COMMANDS
%	Skip this unless you know what you're doing
%----------------------------------------------------------------------------------------

% Header and footer for when a page split occurs within a problem environment
\newcommand{\enterProblemHeader}[1]{
\nobreak\extramarks{#1}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
}

% Header and footer for when a page split occurs between problem environments
\newcommand{\exitProblemHeader}[1]{
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1}{}\nobreak
}

\setcounter{secnumdepth}{0} % Removes default section numbers
\newcounter{homeworkProblemCounter} % Creates a counter to keep track of the number of problems

\newcommand{\homeworkProblemName}{}
\newenvironment{homeworkProblem}[1][Problem \arabic{homeworkProblemCounter}]{ % Makes a new environment called homeworkProblem which takes 1 argument (custom name) but the default is "Problem #"
\stepcounter{homeworkProblemCounter} % Increase counter for number of problems
\renewcommand{\homeworkProblemName}{#1} % Assign \homeworkProblemName the name of the problem
\section{\homeworkProblemName} % Make a section in the document with the custom problem count
\enterProblemHeader{\homeworkProblemName} % Header and footer within the environment
}{
\exitProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

\newcommand{\problemAnswer}[1]{ % Defines the problem answer command with the content as the only argument
\noindent\framebox[\columnwidth][c]{\begin{minipage}{0.98\columnwidth}#1\end{minipage}} % Makes the box around the problem answer and puts the content inside
}

\newcommand{\homeworkSectionName}{}
\newenvironment{homeworkSection}[1]{ % New environment for sections within homework problems, takes 1 argument - the name of the section
\renewcommand{\homeworkSectionName}{#1} % Assign \homeworkSectionName to the name of the section from the environment argument
\subsection{\homeworkSectionName} % Make a subsection with the custom name of the subsection
\enterProblemHeader{\homeworkProblemName\ [\homeworkSectionName]} % Header and footer within the environment
}{
\enterProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

%----------------------------------------------------------------------------------------
%	NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{Assign\#1} % Assignment title
\newcommand{\hmwkDueDate}{Thursday,\ March\ 14th,\ 2018} % Due date
\newcommand{\hmwkClass}{CS299\ Machine Learning} % Course/class
\newcommand{\hmwkClassTime}{6:30 am} % Class/lecture time
\newcommand{\hmwkClassInstructor}{Andrew Ng} % Teacher/lecturer
\newcommand{\hmwkAuthorName}{Bryan Zhang} % Your name


%----------------------------------------------------------------------------------------
% PROBABILITY SHORT CUT
%----------------------------------------------------------------------------------------
\newcommand{\p}[1]{p(#1)}
\newcommand{\abs}[1]{|#1|}
\newcommand{\1}[1]{1\{#1\}}
%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
\vspace{2in}
\textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
\normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
\vspace{0.1in}\large{\textit{\hmwkClassInstructor\ \hmwkClassTime}}
\vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{} % Insert date here if you want it to appear below your name

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

%\setcounter{tocdepth}{1} % Uncomment this line if you don't want subsections listed in the ToC

\newpage
\tableofcontents
\newpage

%----------------------------------------------------------------------------------------
%	PROBLEM 1: Logistic Regression
%----------------------------------------------------------------------------------------

% To have just one problem per page, simply put a \clearpage after each problem

\begin{homeworkProblem}
 \subsection{1(a)}{
 $ H = \nabla \cdot \nabla ^ T J(\theta)$  \par
So $ z^T H z = z^T(\nabla \cdot \nabla^T \cdot J(\theta)) z = (z^T \cdot \nabla)^2 J(\theta) = (z^T \nabla^T \cdot J(\theta))^2 \cdot \frac{1}{J(\theta)}$. \par
Since the first factor of $z^T H z$ is a square term which is definitely bigger than  0, So we focus on the factor $\frac{1}{J(\theta)}$. Because  $J(\theta) = -\frac{1}{m} \sum\limits_{i = 1}^m \log (h_\theta(y^{(i) }x^{(i)})$ and the hypopthesis function is a signoid function ranging from (0,1),the log of the hypothesis funcition must be negative and thus the coost function must be postive. So does the $z^T H z$. \par
q.e.d.
}

\subsection{1(b)}{
The optimized $\theta$ is $[-2.62042271649454,	0.760346235045246,	1.17193037252339]$. \par
\lstinputlisting{logistic_regression_1b.m}
}

\subsection{1(c)} {
\begin{center}
     \includegraphics[width=0.75\columnwidth]{1b.png} % Example image
\end{center}
}
\end{homeworkProblem}

%----------------------------------------------------------------------------------------
%	PROBLEM 2: Poisson regression and the exponential family
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}
\subsection{2(a)}
The possion distribution, $p(y; \lambda) = \frac{e^{-\lambda} \lambda^y}{y!} = \frac{1}{y!} e^(log(\lambda) y - \lambda)$. Compared with the expotential family $p(y;\eta) = b(y) exp(n^T T(y) - a(\eta))$, we can get
\begin{align*}
     b(y) &= \frac{1}{y!} \\
     \eta &= log(\lambda) \\
      T(y) &= y \\
      \alpha(\eta) &= \lambda = e^{\eta}
\end{align*}

\subsection{2(b)}
     The canonical response function gives the mean of the distribution , which in this case, a Possion distribution, is $\lambda$ , in terms of the natural parameter $\eta$. So the canonical response funciton for the Possion distribution is $g(\eta) = e^{\lambda} $.

\subsection{2(c)\&2(d)}
	Here we directly look at the all GLMs instead of the possion case. Our goal is first to derivate the $p(y^{(i)}|x^{(i)};\theta)$ with repect to $\theta_j$ . \par
	For exponential family, $p(y;\eta) = b(y) exp(n^T T(y) - a(\eta))$. After using assumption 3, we subttitute the natural parameter for $\theta^T X$. For training set $$\{(X^{(i)}, Y^{(i)}); i = 1......m\},$$ the distribution is
	\begin{equation}
	   p(y^{(i)}|x^{(i)};\theta) = b(y)exp(\theta^T X^{(i)} -a(\theta^T X^{(i)})).
	\end{equation}

	Then we take derivative of the log-likelyhood above with respect to $\theta_j$, which  shows,
		\begin{equation}
			\begin{split} 
	            & \frac{d}{d \theta_j} log(p(y^{(i)}|x^{(i)};\theta)) \\
	            & = \frac{1}{p(y^{(i)}|x^{(i)};\theta)} \cdot \frac{d}{d \theta_j} p(y^{(i)}|x^{(i)};\theta) \\
	            & =  \frac{1}{p(y^{(i)}|x^{(i)};\theta)} \cdot  (p(y^{(i)}|x^{(i)};\theta)[T(Y^{(i)}) - \frac{d}{d \eta} a(\eta) ] \cdot x_j \\
	             & = (T(Y^{(i)}) - \frac{d}{d \eta} a(\eta)) \cdot x_j
	         \end{split}
		\end{equation}	

	Using the assumption 2, we can get $ T(Y^{(i)} = Y^{(i)} $. Because of this assumption and $\alpha$ being only a hyperparameter, we need only to prove  $\frac{d}{d \eta} a(\eta)$ is the reponse function when natural parameter $\eta$ is replaced by $\theta^T X^{(i)} $ to prove the stochastic gradient ascent has the update rule $$ \theta_j = \theta_j - \alpha(h(x) - y) x_j $$ for every j and looping i until m .
	
	$a(\eta)$ is the log parition function, which is to make sure the priobability funciton can integrate to unity at last. So 
	$$ a(\eta) = log[\int b(Y)exp(\eta^T T(Y) v(dY)]$$

	\begin{equation}
	   \begin{split}
	         & \frac{d}{d \eta^T} a(\eta) =  \frac{\int b(Y)T(Y)exp(\eta^T T(Y) v(dY) }{\int b(Y)exp(\eta^T T(Y) v(dY)} \\
	         & = \int T(Y) [b(Y)exp(\eta^T T(Y)] v(dY) \cdot exp(-log[\int exp(\eta^T T(Y)b(Y) v(dY)]) \\
	        & = \int T(Y) [b(Y)exp(\eta^T T(Y)] v(dY) \cdot exp(-a(\eta)) \\
	        & =  \int T(Y) [b(y)exp(\eta^T T(Y) - a(\eta))] v(dY) \\
	        & = \int T(Y) p(Y|X;\theta) V(dY) \\
	        & = E(T(Y)|X)
	    \end{split}
	\end{equation}
	Since the response function give the estimation of Y ( the T(Y) in most cases) in terms of input features X, so $ \frac{d}{d \eta^T} a(\eta) $ is $ h_\theta(X) $. Thus, we can conclude that the derivative of log-likelyhood given $(X^{(i)}, Y^{(i)})$ with respect to $\theta_j$ is the update rule for stochastic gradient ascent. 
	Since 2(c) is only a special case for 2(d), q.e.d.
\end{homeworkProblem}

%----------------------------------------------------------------------------------------
%	PROBLEM 3: Gaussian discriminant analysis
%---------------------------------------------------------------------------------------homeworkProblem
\begin{homeworkProblem}	
     \subsection{3(a)} 
         Our goal  \footnote{I got this section down by referrencing\par \url{https://duphan.wordpress.com/2016/10/27/gaussian-discriminant-analysis-and-logistic-regression/}} is to prove $p(y= 1|x;\phi,\sum, \mu_{-1}, \mu_1) = \frac{1}{1 + exp(-\theta^Tx - \theta_0)}$. and the similiar form for y = -1.\par
         We can use Bayes's Formula to prove
        \begin{equation}
             \begin{split}
             &  p(y = 1|x; \phi, \Sigma, \mu{-1}, \mu_1) \\
             &= \frac{p(x|y=1) \cdot p(y=1)}{\p{x}} \\
             &= \frac{p(x|y=1) \cdot p(y=1)}{\p{x|y=1} \cdot \p{y  = 1} + \p{x|y=-1} \cdot \p{y=-1}} \\
             &= \frac{1}{1 +  \frac{\p{x|y=-1} \cdot \p{y=-1}}{\p{x|y=1} \cdot \p{y  = 1}}}
             \end{split}	
        \end{equation}
         The form is already similiar to our goal. Next we only need to prove the fraction $\frac{\p{x|y=-1} \cdot \p{y=-1}}{\p{x|y=1} \cdot \p{y  = 1}} = exp(\theta^Tx + \theta_0)$.
         \begin{equation}
             \begin{split}
             &  \frac{\p{x|y=-1} \cdot \p{y=-1}}{\p{x|y=1} \cdot \p{y  = 1}} \\
             &= exp[-1/2(x - \mu_{-1})^T \Sigma^{-1}(x - \mu_{-1}) + 1/2(x - \mu_{1})^T \Sigma^{-1}(x - \mu_{1})] \cdot \frac{1-\phi}{\phi} \\
             &= exp[-1/2(x - \mu_{-1})^T \Sigma^{-1}(x - \mu_{-1}) + 1/2(x - \mu_{1})^T \Sigma^{-1}(x - \mu_{1})] \cdot exp(log(\frac{1-\phi}{\phi })\\
         &= exp[\frac{(\mu_1 - \mu_{-1})^Tx }{\Sigma} - \frac{\mu_{-1}^2 - \mu_1^2}{2 \Sigma} + log(\frac{1 - \phi}{\phi})]
             \end{split}
         \end{equation}
         Campared with $exp(-\theta^Tx + \theta_0)$, we can get
         $$ \theta = \frac{\mu_1 - \mu{-1}}{\Sigma}, $$\par
         $$ \theta_0 = log(\frac{1-\phi}{\phi})  - \frac{\mu_{-1}^2 - \mu_1^2}{2 \Sigma} $$
         Since the fraction is just the inverse in case y = -1, so the form satisfies in both case;
         q.e.d。

    \subsection{3(b) \& 3(c)}
         Our goal  \footnote{I got this section down by referrencing\par \url{http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/LDA.pdf}}is to perform log-likelyhood maximum estimation on the log-likelyhood. By Bayes's rule, 
         \begin{equation}
         	\begin{split}
         		 & l(\phi, \mu_1, \mu_{-1}, \Sigma) \\
         		 &= log\prod_{i =1} ^m \p{x^{(i)}|y^{(i)}; \mu_{-1}, \mu_1, \Sigma}\cdot \p{y^{(i)}} \\
         		\footnote{\1{y = i} is the notation introduced in the lecture notes 1 page 27.} &= \sum_{i = 1}^{m} log\{ [\phi \cdot N(x^{(i)}|\mu_1, \Sigma)]^{\1{y^{(i)} = 1}}  \cdot   [(1-\phi)\cdot N(x^{(i)}|\mu_{-1}, \Sigma)]^{\1{y^{(i)} = -1}}\} \\
         		 &= \sum_{i = 1}^m \{ y^{(i)} \}
         	\end{split}
         \end{equation}
\end{homeworkProblem}

%----------------------------------------------------------------------------------------

\end{document}